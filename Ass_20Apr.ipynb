{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec24d6c0-9bd3-435a-ba7c-3635d8d56111",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39131b7e-b9a7-48c1-93fe-5a54fcd4b69d",
   "metadata": {},
   "source": [
    "Sol : The K-Nearest Neighbor (KNN) algorithm is a popular machine learning technique used for classification and regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
    "\n",
    "During the training phase, the KNN algorithm stores the entire training dataset as a reference. When making predictions, it calculates the distance between the input data point and all the training examples, using a chosen distance metric such as Euclidean distance.\n",
    "\n",
    "Next, the algorithm identifies the K nearest neighbors to the input data point based on their distances. In the case of classification, the algorithm assigns the most common class label among the K neighbors as the predicted label for the input data point. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a4026-11cc-49d2-9945-16876d01b6ac",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9258c7-dc4c-4db1-8e32-c6216551e61a",
   "metadata": {},
   "source": [
    "Sol : To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7cb8a-29d1-4d9f-8d97-b2a33bdba068",
   "metadata": {},
   "source": [
    "## Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989d277-3089-4e61-8c08-72378c0729b6",
   "metadata": {},
   "source": [
    "Sol : KNN Classifier predict values of a classification problems having 2 or more classes in output feature while KNN regressor predict continous values of a regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bd9b6-1430-46f2-83dc-1a66639ce9ed",
   "metadata": {},
   "source": [
    "## Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7db66-3f4f-415f-b39d-b15e9343b59f",
   "metadata": {},
   "source": [
    "Sol : Performance of KNN can be measured by accuracy_score and confusion amtrix in case of KNN classifier and mse,r2_score for KNN regressor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1db2-3a42-4131-b9b7-36208810fd44",
   "metadata": {},
   "source": [
    "## Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a4d9d-208c-4443-aaba-acde6d923268",
   "metadata": {},
   "source": [
    "Sol : KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded26867-107d-40cb-b2ef-fa9c2d35f0a1",
   "metadata": {},
   "source": [
    "## Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936353f-d3ac-4079-b539-bf9a2dc8d65c",
   "metadata": {},
   "source": [
    "Sol : The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a8733-89ab-4fb2-8db3-d1981bc0ac6d",
   "metadata": {},
   "source": [
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ed399-cc7a-4ce4-884f-e8fa0cb3d00c",
   "metadata": {},
   "source": [
    "Sol : KNN classifier is better for classification problems whereas KNN regressor is better for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d72c-41e4-4494-843f-1e8db788fe78",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f5a054-a05c-4dfb-9090-21e63d34f81c",
   "metadata": {},
   "source": [
    "Strenghts:\n",
    "- Quick calculation time\n",
    "- Simple algorithm – to interpret \n",
    "- Versatile – useful for regression and classification\n",
    "- High accuracy – you do not need to compare with better-supervised learning models\n",
    "- No assumptions about data – no need to make additional assumptions, tune several parameters, or build a model. This makes it crucial in nonlinear data case. \n",
    "\n",
    "Weaknesses:\n",
    "- Accuracy depends on the quality of the data\n",
    "- With large data, the prediction stage might be slow\n",
    "- Sensitive to the scale of the data and irrelevant features\n",
    "- Require high memory – need to store all of the training data\n",
    "- Given that it stores all of the training, it can be computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81094c-cf08-4b75-bcb2-7eb76ffa5643",
   "metadata": {},
   "source": [
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657db42-51d7-44a0-b090-17089df7b8c1",
   "metadata": {},
   "source": [
    "Sol : Manhattan distance captures the distance between two points by aggregating the pairwise absolute difference between each variable while Euclidean distance captures the same by aggregating the squared difference in each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcef1c7-302a-44f2-8057-8f1e14a396d3",
   "metadata": {},
   "source": [
    "## Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54cad5-eef4-4ec3-b2de-bb4602c6fa5a",
   "metadata": {},
   "source": [
    "Sol : Feature scaling is crucial for the KNN algorithm, as it helps in preventing features with larger magnitudes from dominating the distance calculations.Features with larger magnitudes can disproportionately influence the distance calculation, leading to biased results.\n",
    "Feature scaling addresses this issue by transforming the features to a comparable range or scale, ensuring that each feature contributes fairly to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb05e9-f649-4a04-ba8a-b88a25fe2020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
